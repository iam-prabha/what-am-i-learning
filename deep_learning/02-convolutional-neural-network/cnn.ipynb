{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a4fb572",
   "metadata": {},
   "source": [
    "# Convolution Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a64d5e",
   "metadata": {},
   "source": [
    "- A Convelutional neural network (convNet/CNN) is deep learning algorithm which takes in an inputs image, assign importance (learnable weigths and biases) to various aspects/objects in the image and be able to differetiate one from the other. \n",
    "- The pre-processing required in a convNet is much lower as compared to other classification algorithms.\n",
    "- While in primitive methods filters are hand-engineered, with enough training, convNets have the ability to learn these filters/characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90859077",
   "metadata": {},
   "source": [
    "![convolution layer.png](https://www.kaggleusercontent.com/kf/99705616/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..mS32FYLYwSsMSYO5vJAuZg.cXnDOwrGdtNgp8rFz2msenOptZMX1EkbxHcpivlCbaZ8bCZGfGkQWCS1XxhjMEFc_tfDkA-vchSRIrVp0_Det0nPjIY2ULmSaJl4K1qh4dgnYITyJKh5aLqu6e-cTzYTI3Z_9VH3RKrFoHRSgtRFOX5nHhD0W0N1twKKCXt9FOtoaPtILmefWCaj5jLwilnPc51SLU4YV-fhMkzPsBbXPS-NwCOlVGh9ro4D9BqmVeh2CLnfr4ob9C5oshfYqSUrj59HjhF596oM6HLqMv3Qvo1aee5YAWsQQaIHomc78ET9Pc08Cq_cgH-HYpMpxdBQSHTaTqexErxZZ3e4ChSZT5ehUQCX7pInt0ShsLOIO2VVDQUHzVx1Q1iGaZCQXpfY1xIFQpEw7MlW-z7J0RRFvaiscyyT6rZKdpGxQJBi2OReFAt7ubyRFEfMJ-MCzgkiOV9xtN84smPA5NQqCS16TVnQeBAUS79fkbpGXm2SbjSrtZgsiYVHmH6KFxrBDj8_W-Wg8p1u9sbZ-ry0ZjFRn4BUDy5XgK-wv-Lv749mWggYyGFRoO3FALi8DLCz7wdEVow-MndI3C3bdCUa2eUSfKctOCQntGoh6JvYEvVNRi2aACQJ5xHdyOOWEj84QCUOJd8vSSRoPZDMW6ycYZA9dYDOylHhwZlpPvl313VQeKA.ai3Ue7GYk7OizC8bppsMUA/__results___files/__results___45_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af71b5af",
   "metadata": {},
   "source": [
    "![con2.img](https://www.kaggleusercontent.com/kf/99705616/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..mS32FYLYwSsMSYO5vJAuZg.cXnDOwrGdtNgp8rFz2msenOptZMX1EkbxHcpivlCbaZ8bCZGfGkQWCS1XxhjMEFc_tfDkA-vchSRIrVp0_Det0nPjIY2ULmSaJl4K1qh4dgnYITyJKh5aLqu6e-cTzYTI3Z_9VH3RKrFoHRSgtRFOX5nHhD0W0N1twKKCXt9FOtoaPtILmefWCaj5jLwilnPc51SLU4YV-fhMkzPsBbXPS-NwCOlVGh9ro4D9BqmVeh2CLnfr4ob9C5oshfYqSUrj59HjhF596oM6HLqMv3Qvo1aee5YAWsQQaIHomc78ET9Pc08Cq_cgH-HYpMpxdBQSHTaTqexErxZZ3e4ChSZT5ehUQCX7pInt0ShsLOIO2VVDQUHzVx1Q1iGaZCQXpfY1xIFQpEw7MlW-z7J0RRFvaiscyyT6rZKdpGxQJBi2OReFAt7ubyRFEfMJ-MCzgkiOV9xtN84smPA5NQqCS16TVnQeBAUS79fkbpGXm2SbjSrtZgsiYVHmH6KFxrBDj8_W-Wg8p1u9sbZ-ry0ZjFRn4BUDy5XgK-wv-Lv749mWggYyGFRoO3FALi8DLCz7wdEVow-MndI3C3bdCUa2eUSfKctOCQntGoh6JvYEvVNRi2aACQJ5xHdyOOWEj84QCUOJd8vSSRoPZDMW6ycYZA9dYDOylHhwZlpPvl313VQeKA.ai3Ue7GYk7OizC8bppsMUA/__results___files/__results___46_0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c0173fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd8d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv:\n",
    "\n",
    "    def __init__(self, num_filters):\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        # why diviide by 9...Xavier initialization\n",
    "        self.filters = np.random.randn(num_filters, 3, 3) / 9\n",
    "\n",
    "    def iterate_regions(self, image):\n",
    "        #generate all possible 3*3 image regions using valid padding\n",
    "\n",
    "        h, w = image.shape\n",
    "\n",
    "        for i in range(h-2):\n",
    "            for j in range(w-2):\n",
    "                im_region = image[i: (i+3), j:(j+3)]\n",
    "                yield im_region, i, j\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.last_input = input\n",
    "\n",
    "        h,w = input.shape\n",
    "\n",
    "        output = np.zeros((h-2, w-2, self.num_filters))\n",
    "\n",
    "        for im_regions, i ,j in self.iterate_regions(input):\n",
    "            output[i, j] = np.sum(im_regions * self.filters, axis=(1,2))\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def backprop(self, d_l_d_out, learn_rate):\n",
    "        '''\n",
    "        performs a backward pas of the conv layer.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        - learn_rate is a float.\n",
    "        '''\n",
    "        d_l_d_filters = np.zeros(self.filters.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "            for f in range(self.num_filters):\n",
    "                d_l_d_filters[f] += d_l_d_out[i, j ,f] * im_region\n",
    "\n",
    "            #update filters\n",
    "            self.filters -= learn_rate * d_l_d_filters\n",
    "\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c86f83b",
   "metadata": {},
   "source": [
    "# Maxpooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f33eff2",
   "metadata": {},
   "source": [
    "- A max pooling layer can't be trained it doesn't actually have any weights, but we still need  to implement a backprop() method for it to calculate gradients. We’ll start by adding forward phase caching again. All we need to cache this time is the input:\n",
    "\n",
    "- During the forward pass, the Max Pooling layer takes an input volume and halves its width and height dimensions by picking the max values over 2x2 blocks. The backward pass does the opposite: we’ll double the width and height of the loss gradient by assigning each gradient value to where the original max value was in its corresponding 2x2 block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aa6a28",
   "metadata": {},
   "source": [
    "![maxpool.png](https://www.kaggleusercontent.com/kf/99705616/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..mS32FYLYwSsMSYO5vJAuZg.cXnDOwrGdtNgp8rFz2msenOptZMX1EkbxHcpivlCbaZ8bCZGfGkQWCS1XxhjMEFc_tfDkA-vchSRIrVp0_Det0nPjIY2ULmSaJl4K1qh4dgnYITyJKh5aLqu6e-cTzYTI3Z_9VH3RKrFoHRSgtRFOX5nHhD0W0N1twKKCXt9FOtoaPtILmefWCaj5jLwilnPc51SLU4YV-fhMkzPsBbXPS-NwCOlVGh9ro4D9BqmVeh2CLnfr4ob9C5oshfYqSUrj59HjhF596oM6HLqMv3Qvo1aee5YAWsQQaIHomc78ET9Pc08Cq_cgH-HYpMpxdBQSHTaTqexErxZZ3e4ChSZT5ehUQCX7pInt0ShsLOIO2VVDQUHzVx1Q1iGaZCQXpfY1xIFQpEw7MlW-z7J0RRFvaiscyyT6rZKdpGxQJBi2OReFAt7ubyRFEfMJ-MCzgkiOV9xtN84smPA5NQqCS16TVnQeBAUS79fkbpGXm2SbjSrtZgsiYVHmH6KFxrBDj8_W-Wg8p1u9sbZ-ry0ZjFRn4BUDy5XgK-wv-Lv749mWggYyGFRoO3FALi8DLCz7wdEVow-MndI3C3bdCUa2eUSfKctOCQntGoh6JvYEvVNRi2aACQJ5xHdyOOWEj84QCUOJd8vSSRoPZDMW6ycYZA9dYDOylHhwZlpPvl313VQeKA.ai3Ue7GYk7OizC8bppsMUA/__results___files/__results___49_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4940a6",
   "metadata": {},
   "source": [
    "![example.png](https://www.kaggleusercontent.com/kf/99705616/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..mS32FYLYwSsMSYO5vJAuZg.cXnDOwrGdtNgp8rFz2msenOptZMX1EkbxHcpivlCbaZ8bCZGfGkQWCS1XxhjMEFc_tfDkA-vchSRIrVp0_Det0nPjIY2ULmSaJl4K1qh4dgnYITyJKh5aLqu6e-cTzYTI3Z_9VH3RKrFoHRSgtRFOX5nHhD0W0N1twKKCXt9FOtoaPtILmefWCaj5jLwilnPc51SLU4YV-fhMkzPsBbXPS-NwCOlVGh9ro4D9BqmVeh2CLnfr4ob9C5oshfYqSUrj59HjhF596oM6HLqMv3Qvo1aee5YAWsQQaIHomc78ET9Pc08Cq_cgH-HYpMpxdBQSHTaTqexErxZZ3e4ChSZT5ehUQCX7pInt0ShsLOIO2VVDQUHzVx1Q1iGaZCQXpfY1xIFQpEw7MlW-z7J0RRFvaiscyyT6rZKdpGxQJBi2OReFAt7ubyRFEfMJ-MCzgkiOV9xtN84smPA5NQqCS16TVnQeBAUS79fkbpGXm2SbjSrtZgsiYVHmH6KFxrBDj8_W-Wg8p1u9sbZ-ry0ZjFRn4BUDy5XgK-wv-Lv749mWggYyGFRoO3FALi8DLCz7wdEVow-MndI3C3bdCUa2eUSfKctOCQntGoh6JvYEvVNRi2aACQJ5xHdyOOWEj84QCUOJd8vSSRoPZDMW6ycYZA9dYDOylHhwZlpPvl313VQeKA.ai3Ue7GYk7OizC8bppsMUA/__results___files/__results___50_0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d697c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "    def iterate_regions(self, image):\n",
    "        h , w , _ = image.shape\n",
    "\n",
    "        new_h = h // 2\n",
    "        new_w = w // 2\n",
    "\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i*2): (i*2+2), (j*2): (j*2+2)]\n",
    "                yield im_region , i ,j\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        self.last_input = input\n",
    "        \n",
    "        h, w, num_filters = input.shape\n",
    "        output = np.zeros((h//2, w//2, num_filters))\n",
    "        \n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i,j] = np.amax(im_region,axis=(0,1))\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def backprop(self, d_l_d_out):\n",
    "        '''\n",
    "        Performs a backward pass of the maxpool layer.\n",
    "        Returns the loss gradient for this layer's inputs.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        '''\n",
    "        d_l_d_input = np.zeros(self.last_input.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "            h, w, f = im_region.shape\n",
    "            amax = np.amax(im_region, axis=(0,1))\n",
    "\n",
    "            for i2 in range(h):\n",
    "                for j2 in range(w):\n",
    "                    for f2 in range(f):\n",
    "                        #if the pixel was the max value, copy the gradient to it\n",
    "                        if(im_region[i2,j2,f2] == amax[f2]):\n",
    "                            d_l_d_input[i*2+i2, j*2+j2 ,f2] = d_l_d_out[i, j, f2]\n",
    "                            break;\n",
    "        return d_l_d_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0be28c",
   "metadata": {},
   "source": [
    "# softmax layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7acb6a",
   "metadata": {},
   "source": [
    "![softmax layer](https://www.kaggleusercontent.com/kf/99705616/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..mS32FYLYwSsMSYO5vJAuZg.cXnDOwrGdtNgp8rFz2msenOptZMX1EkbxHcpivlCbaZ8bCZGfGkQWCS1XxhjMEFc_tfDkA-vchSRIrVp0_Det0nPjIY2ULmSaJl4K1qh4dgnYITyJKh5aLqu6e-cTzYTI3Z_9VH3RKrFoHRSgtRFOX5nHhD0W0N1twKKCXt9FOtoaPtILmefWCaj5jLwilnPc51SLU4YV-fhMkzPsBbXPS-NwCOlVGh9ro4D9BqmVeh2CLnfr4ob9C5oshfYqSUrj59HjhF596oM6HLqMv3Qvo1aee5YAWsQQaIHomc78ET9Pc08Cq_cgH-HYpMpxdBQSHTaTqexErxZZ3e4ChSZT5ehUQCX7pInt0ShsLOIO2VVDQUHzVx1Q1iGaZCQXpfY1xIFQpEw7MlW-z7J0RRFvaiscyyT6rZKdpGxQJBi2OReFAt7ubyRFEfMJ-MCzgkiOV9xtN84smPA5NQqCS16TVnQeBAUS79fkbpGXm2SbjSrtZgsiYVHmH6KFxrBDj8_W-Wg8p1u9sbZ-ry0ZjFRn4BUDy5XgK-wv-Lv749mWggYyGFRoO3FALi8DLCz7wdEVow-MndI3C3bdCUa2eUSfKctOCQntGoh6JvYEvVNRi2aACQJ5xHdyOOWEj84QCUOJd8vSSRoPZDMW6ycYZA9dYDOylHhwZlpPvl313VQeKA.ai3Ue7GYk7OizC8bppsMUA/__results___files/__results___53_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc7843",
   "metadata": {},
   "source": [
    "![gradient](https://www.kaggleusercontent.com/kf/99705616/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..mS32FYLYwSsMSYO5vJAuZg.cXnDOwrGdtNgp8rFz2msenOptZMX1EkbxHcpivlCbaZ8bCZGfGkQWCS1XxhjMEFc_tfDkA-vchSRIrVp0_Det0nPjIY2ULmSaJl4K1qh4dgnYITyJKh5aLqu6e-cTzYTI3Z_9VH3RKrFoHRSgtRFOX5nHhD0W0N1twKKCXt9FOtoaPtILmefWCaj5jLwilnPc51SLU4YV-fhMkzPsBbXPS-NwCOlVGh9ro4D9BqmVeh2CLnfr4ob9C5oshfYqSUrj59HjhF596oM6HLqMv3Qvo1aee5YAWsQQaIHomc78ET9Pc08Cq_cgH-HYpMpxdBQSHTaTqexErxZZ3e4ChSZT5ehUQCX7pInt0ShsLOIO2VVDQUHzVx1Q1iGaZCQXpfY1xIFQpEw7MlW-z7J0RRFvaiscyyT6rZKdpGxQJBi2OReFAt7ubyRFEfMJ-MCzgkiOV9xtN84smPA5NQqCS16TVnQeBAUS79fkbpGXm2SbjSrtZgsiYVHmH6KFxrBDj8_W-Wg8p1u9sbZ-ry0ZjFRn4BUDy5XgK-wv-Lv749mWggYyGFRoO3FALi8DLCz7wdEVow-MndI3C3bdCUa2eUSfKctOCQntGoh6JvYEvVNRi2aACQJ5xHdyOOWEj84QCUOJd8vSSRoPZDMW6ycYZA9dYDOylHhwZlpPvl313VQeKA.ai3Ue7GYk7OizC8bppsMUA/__results___files/__results___54_0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c92ac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self, input_len, nodes):\n",
    "        # We divide by input_len to reduce the variance of our initial values\n",
    "        self.weights = np.random.randn(input_len, nodes)/input_len\n",
    "        self.biases = np.zeros(nodes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        self.last_input_shape = input.shape\n",
    "        \n",
    "        input = input.flatten()\n",
    "        self.last_input = input\n",
    "        \n",
    "        input_len, nodes = self.weights.shape\n",
    "        \n",
    "        totals = np.dot(input, self.weights) + self.biases\n",
    "        self.last_totals = totals\n",
    "        \n",
    "        exp = np.exp(totals)\n",
    "        return(exp/np.sum(exp, axis=0)) \n",
    "    \n",
    "    def backprop(self, d_l_d_out, learn_rate):\n",
    "        \"\"\"  \n",
    "        Performs a backward pass of the softmax layer.\n",
    "        Returns the loss gradient for this layers inputs.\n",
    "        - d_L_d_out is the loss gradient for this layers outputs.\n",
    "        \"\"\"\n",
    "        \n",
    "        #We know only 1 element of d_l_d_out will be nonzero\n",
    "        for i, gradient in enumerate(d_l_d_out):\n",
    "            if(gradient == 0):\n",
    "                continue\n",
    "            \n",
    "            #e^totals\n",
    "            t_exp = np.exp(self.last_totals)\n",
    "            \n",
    "            #Sum of all e^totals\n",
    "            S = np.sum(t_exp)\n",
    "            \n",
    "            #gradients of out[i] against totals\n",
    "            d_out_d_t = -t_exp[i] * t_exp/ (S**2)\n",
    "            d_out_d_t[i] = t_exp[i] * (S-t_exp[i]) /(S**2)\n",
    "            \n",
    "            # Gradients of totals against weights/biases/input\n",
    "            d_t_d_w = self.last_input\n",
    "            d_t_d_b = 1\n",
    "            d_t_d_inputs = self.weights\n",
    "            \n",
    "            #Gradients of loss against totals\n",
    "            d_l_d_t = gradient * d_out_d_t\n",
    "            \n",
    "            #Gradients of loss against weights/biases/input\n",
    "            d_l_d_w = d_t_d_w[np.newaxis].T @ d_l_d_t[np.newaxis]\n",
    "            d_l_d_b = d_l_d_t * d_t_d_b  \n",
    "            d_l_d_inputs = d_t_d_inputs @ d_l_d_t\n",
    "            \n",
    "            #update weights/biases\n",
    "            self.weights -= learn_rate * d_l_d_w\n",
    "            self.biases -= learn_rate * d_l_d_b\n",
    "            return d_l_d_inputs.reshape(self.last_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ddff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Download and load the MNIST dataset using PyTorch\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Get numpy arrays like the original API, normalizing to 0-255 range\n",
    "train_images = mnist_train.data[:1000].numpy()\n",
    "train_labels = mnist_train.targets[:1000].numpy()\n",
    "test_images = mnist_test.data[:1000].numpy()\n",
    "test_labels = mnist_test.targets[:1000].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba3be4d",
   "metadata": {},
   "source": [
    "![cnn-train.png](https://www.kaggleusercontent.com/kf/99705616/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..mS32FYLYwSsMSYO5vJAuZg.cXnDOwrGdtNgp8rFz2msenOptZMX1EkbxHcpivlCbaZ8bCZGfGkQWCS1XxhjMEFc_tfDkA-vchSRIrVp0_Det0nPjIY2ULmSaJl4K1qh4dgnYITyJKh5aLqu6e-cTzYTI3Z_9VH3RKrFoHRSgtRFOX5nHhD0W0N1twKKCXt9FOtoaPtILmefWCaj5jLwilnPc51SLU4YV-fhMkzPsBbXPS-NwCOlVGh9ro4D9BqmVeh2CLnfr4ob9C5oshfYqSUrj59HjhF596oM6HLqMv3Qvo1aee5YAWsQQaIHomc78ET9Pc08Cq_cgH-HYpMpxdBQSHTaTqexErxZZ3e4ChSZT5ehUQCX7pInt0ShsLOIO2VVDQUHzVx1Q1iGaZCQXpfY1xIFQpEw7MlW-z7J0RRFvaiscyyT6rZKdpGxQJBi2OReFAt7ubyRFEfMJ-MCzgkiOV9xtN84smPA5NQqCS16TVnQeBAUS79fkbpGXm2SbjSrtZgsiYVHmH6KFxrBDj8_W-Wg8p1u9sbZ-ry0ZjFRn4BUDy5XgK-wv-Lv749mWggYyGFRoO3FALi8DLCz7wdEVow-MndI3C3bdCUa2eUSfKctOCQntGoh6JvYEvVNRi2aACQJ5xHdyOOWEj84QCUOJd8vSSRoPZDMW6ycYZA9dYDOylHhwZlpPvl313VQeKA.ai3Ue7GYk7OizC8bppsMUA/__results___files/__results___58_0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fc8b33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST CNN initialized\n",
      "----EPOCH 1 ---\n",
      "[Step 100] Past 100 steps: Average Loss 2.193 | Accuracy: 20%\n",
      "[Step 200] Past 100 steps: Average Loss 2.077 | Accuracy: 38%\n",
      "[Step 300] Past 100 steps: Average Loss 1.879 | Accuracy: 49%\n",
      "[Step 400] Past 100 steps: Average Loss 1.781 | Accuracy: 47%\n",
      "[Step 500] Past 100 steps: Average Loss 1.635 | Accuracy: 64%\n",
      "[Step 600] Past 100 steps: Average Loss 1.552 | Accuracy: 68%\n",
      "[Step 700] Past 100 steps: Average Loss 1.510 | Accuracy: 66%\n",
      "[Step 800] Past 100 steps: Average Loss 1.333 | Accuracy: 74%\n",
      "[Step 900] Past 100 steps: Average Loss 1.299 | Accuracy: 71%\n",
      "[Step 1000] Past 100 steps: Average Loss 1.251 | Accuracy: 73%\n",
      "----EPOCH 2 ---\n",
      "[Step 100] Past 100 steps: Average Loss 1.127 | Accuracy: 71%\n",
      "[Step 200] Past 100 steps: Average Loss 1.151 | Accuracy: 81%\n",
      "[Step 300] Past 100 steps: Average Loss 1.045 | Accuracy: 85%\n",
      "[Step 400] Past 100 steps: Average Loss 1.020 | Accuracy: 79%\n",
      "[Step 500] Past 100 steps: Average Loss 1.028 | Accuracy: 82%\n",
      "[Step 600] Past 100 steps: Average Loss 1.069 | Accuracy: 70%\n",
      "[Step 700] Past 100 steps: Average Loss 0.929 | Accuracy: 81%\n",
      "[Step 800] Past 100 steps: Average Loss 0.836 | Accuracy: 87%\n",
      "[Step 900] Past 100 steps: Average Loss 0.897 | Accuracy: 80%\n",
      "[Step 1000] Past 100 steps: Average Loss 1.006 | Accuracy: 73%\n",
      "----EPOCH 3 ---\n",
      "[Step 100] Past 100 steps: Average Loss 0.947 | Accuracy: 75%\n",
      "[Step 200] Past 100 steps: Average Loss 0.678 | Accuracy: 85%\n",
      "[Step 300] Past 100 steps: Average Loss 0.793 | Accuracy: 85%\n",
      "[Step 400] Past 100 steps: Average Loss 0.725 | Accuracy: 88%\n",
      "[Step 500] Past 100 steps: Average Loss 0.907 | Accuracy: 77%\n",
      "[Step 600] Past 100 steps: Average Loss 0.842 | Accuracy: 79%\n",
      "[Step 700] Past 100 steps: Average Loss 0.791 | Accuracy: 85%\n",
      "[Step 800] Past 100 steps: Average Loss 0.689 | Accuracy: 91%\n",
      "[Step 900] Past 100 steps: Average Loss 0.638 | Accuracy: 86%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.851 | Accuracy: 80%\n"
     ]
    }
   ],
   "source": [
    "conv = Conv(8)\n",
    "pool = MaxPool()\n",
    "softmax = Softmax(13 * 13 * 8, 10)\n",
    "\n",
    "def forward(image, label):\n",
    "    # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier\n",
    "    # to work with. This is standard practice.\n",
    "    \n",
    "    out = conv.forward((image/255) - 0.5)\n",
    "    out = pool.forward(out)\n",
    "    out = softmax.forward(out)\n",
    "    \n",
    "    #calculate cross-entropy loss and accuracy\n",
    "    loss = -np.log(out[label])\n",
    "    acc = 1 if(np.argmax(out) == label) else 0\n",
    "    \n",
    "    return out, loss, acc\n",
    "\n",
    "\n",
    "def train(im, label, lr=0.005):\n",
    "    #forward\n",
    "    out,loss,acc = forward(im, label)\n",
    "    \n",
    "    #calculate initial gradient\n",
    "    gradient = np.zeros(10)\n",
    "    gradient[label] = -1/out[label]\n",
    "    \n",
    "    \n",
    "    #Backprop\n",
    "    gradient = softmax.backprop(gradient, lr)\n",
    "    gradient = pool.backprop(gradient)\n",
    "    gradient = conv.backprop(gradient, lr)\n",
    "    \n",
    "    return loss, acc\n",
    "    \n",
    "    \n",
    "print('MNIST CNN initialized')\n",
    "\n",
    "for epoch in range(3):\n",
    "    print('----EPOCH %d ---'%(epoch+1))\n",
    "    \n",
    "    #shuffle the training data\n",
    "    permutation = np.random.permutation(len(train_images))\n",
    "    train_images = train_images[permutation]\n",
    "    train_labels = train_labels[permutation]\n",
    "\n",
    "\n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    for i, (im, label) in enumerate(zip(train_images, train_labels)):\n",
    "\n",
    "        #print stats every 100 steps\n",
    "        if(i>0 and i %100 == 99):\n",
    "            print('[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %(i + 1, loss / 100, num_correct))\n",
    "\n",
    "            # Reset loss and num_correct counters after every 100 steps to correctly track average loss and accuracy for each reporting interval.\n",
    "            # This avoids counters accumulating over the entire epoch, providing accurate per-interval statistics.\n",
    "            loss = 0\n",
    "            num_correct = 0\n",
    "        l, acc = train(im, label)\n",
    "        loss += l\n",
    "        num_correct += acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4ba1c",
   "metadata": {},
   "source": [
    "![backpropagation-indepth.png](https://www.kaggleusercontent.com/kf/99705616/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..mS32FYLYwSsMSYO5vJAuZg.cXnDOwrGdtNgp8rFz2msenOptZMX1EkbxHcpivlCbaZ8bCZGfGkQWCS1XxhjMEFc_tfDkA-vchSRIrVp0_Det0nPjIY2ULmSaJl4K1qh4dgnYITyJKh5aLqu6e-cTzYTI3Z_9VH3RKrFoHRSgtRFOX5nHhD0W0N1twKKCXt9FOtoaPtILmefWCaj5jLwilnPc51SLU4YV-fhMkzPsBbXPS-NwCOlVGh9ro4D9BqmVeh2CLnfr4ob9C5oshfYqSUrj59HjhF596oM6HLqMv3Qvo1aee5YAWsQQaIHomc78ET9Pc08Cq_cgH-HYpMpxdBQSHTaTqexErxZZ3e4ChSZT5ehUQCX7pInt0ShsLOIO2VVDQUHzVx1Q1iGaZCQXpfY1xIFQpEw7MlW-z7J0RRFvaiscyyT6rZKdpGxQJBi2OReFAt7ubyRFEfMJ-MCzgkiOV9xtN84smPA5NQqCS16TVnQeBAUS79fkbpGXm2SbjSrtZgsiYVHmH6KFxrBDj8_W-Wg8p1u9sbZ-ry0ZjFRn4BUDy5XgK-wv-Lv749mWggYyGFRoO3FALi8DLCz7wdEVow-MndI3C3bdCUa2eUSfKctOCQntGoh6JvYEvVNRi2aACQJ5xHdyOOWEj84QCUOJd8vSSRoPZDMW6ycYZA9dYDOylHhwZlpPvl313VQeKA.ai3Ue7GYk7OizC8bppsMUA/__results___files/__results___61_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3273ecf",
   "metadata": {},
   "source": [
    "![backpropagation-indepth.png](https://www.kaggleusercontent.com/kf/99705616/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..mS32FYLYwSsMSYO5vJAuZg.cXnDOwrGdtNgp8rFz2msenOptZMX1EkbxHcpivlCbaZ8bCZGfGkQWCS1XxhjMEFc_tfDkA-vchSRIrVp0_Det0nPjIY2ULmSaJl4K1qh4dgnYITyJKh5aLqu6e-cTzYTI3Z_9VH3RKrFoHRSgtRFOX5nHhD0W0N1twKKCXt9FOtoaPtILmefWCaj5jLwilnPc51SLU4YV-fhMkzPsBbXPS-NwCOlVGh9ro4D9BqmVeh2CLnfr4ob9C5oshfYqSUrj59HjhF596oM6HLqMv3Qvo1aee5YAWsQQaIHomc78ET9Pc08Cq_cgH-HYpMpxdBQSHTaTqexErxZZ3e4ChSZT5ehUQCX7pInt0ShsLOIO2VVDQUHzVx1Q1iGaZCQXpfY1xIFQpEw7MlW-z7J0RRFvaiscyyT6rZKdpGxQJBi2OReFAt7ubyRFEfMJ-MCzgkiOV9xtN84smPA5NQqCS16TVnQeBAUS79fkbpGXm2SbjSrtZgsiYVHmH6KFxrBDj8_W-Wg8p1u9sbZ-ry0ZjFRn4BUDy5XgK-wv-Lv749mWggYyGFRoO3FALi8DLCz7wdEVow-MndI3C3bdCUa2eUSfKctOCQntGoh6JvYEvVNRi2aACQJ5xHdyOOWEj84QCUOJd8vSSRoPZDMW6ycYZA9dYDOylHhwZlpPvl313VQeKA.ai3Ue7GYk7OizC8bppsMUA/__results___files/__results___62_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafac182",
   "metadata": {},
   "source": [
    "![backpropagation-indepth](https://www.kaggleusercontent.com/kf/99705616/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..mS32FYLYwSsMSYO5vJAuZg.cXnDOwrGdtNgp8rFz2msenOptZMX1EkbxHcpivlCbaZ8bCZGfGkQWCS1XxhjMEFc_tfDkA-vchSRIrVp0_Det0nPjIY2ULmSaJl4K1qh4dgnYITyJKh5aLqu6e-cTzYTI3Z_9VH3RKrFoHRSgtRFOX5nHhD0W0N1twKKCXt9FOtoaPtILmefWCaj5jLwilnPc51SLU4YV-fhMkzPsBbXPS-NwCOlVGh9ro4D9BqmVeh2CLnfr4ob9C5oshfYqSUrj59HjhF596oM6HLqMv3Qvo1aee5YAWsQQaIHomc78ET9Pc08Cq_cgH-HYpMpxdBQSHTaTqexErxZZ3e4ChSZT5ehUQCX7pInt0ShsLOIO2VVDQUHzVx1Q1iGaZCQXpfY1xIFQpEw7MlW-z7J0RRFvaiscyyT6rZKdpGxQJBi2OReFAt7ubyRFEfMJ-MCzgkiOV9xtN84smPA5NQqCS16TVnQeBAUS79fkbpGXm2SbjSrtZgsiYVHmH6KFxrBDj8_W-Wg8p1u9sbZ-ry0ZjFRn4BUDy5XgK-wv-Lv749mWggYyGFRoO3FALi8DLCz7wdEVow-MndI3C3bdCUa2eUSfKctOCQntGoh6JvYEvVNRi2aACQJ5xHdyOOWEj84QCUOJd8vSSRoPZDMW6ycYZA9dYDOylHhwZlpPvl313VQeKA.ai3Ue7GYk7OizC8bppsMUA/__results___files/__results___63_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242b1609",
   "metadata": {},
   "source": [
    "![backpropagation-indepth.png](https://www.kaggleusercontent.com/kf/99705616/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..mS32FYLYwSsMSYO5vJAuZg.cXnDOwrGdtNgp8rFz2msenOptZMX1EkbxHcpivlCbaZ8bCZGfGkQWCS1XxhjMEFc_tfDkA-vchSRIrVp0_Det0nPjIY2ULmSaJl4K1qh4dgnYITyJKh5aLqu6e-cTzYTI3Z_9VH3RKrFoHRSgtRFOX5nHhD0W0N1twKKCXt9FOtoaPtILmefWCaj5jLwilnPc51SLU4YV-fhMkzPsBbXPS-NwCOlVGh9ro4D9BqmVeh2CLnfr4ob9C5oshfYqSUrj59HjhF596oM6HLqMv3Qvo1aee5YAWsQQaIHomc78ET9Pc08Cq_cgH-HYpMpxdBQSHTaTqexErxZZ3e4ChSZT5ehUQCX7pInt0ShsLOIO2VVDQUHzVx1Q1iGaZCQXpfY1xIFQpEw7MlW-z7J0RRFvaiscyyT6rZKdpGxQJBi2OReFAt7ubyRFEfMJ-MCzgkiOV9xtN84smPA5NQqCS16TVnQeBAUS79fkbpGXm2SbjSrtZgsiYVHmH6KFxrBDj8_W-Wg8p1u9sbZ-ry0ZjFRn4BUDy5XgK-wv-Lv749mWggYyGFRoO3FALi8DLCz7wdEVow-MndI3C3bdCUa2eUSfKctOCQntGoh6JvYEvVNRi2aACQJ5xHdyOOWEj84QCUOJd8vSSRoPZDMW6ycYZA9dYDOylHhwZlpPvl313VQeKA.ai3Ue7GYk7OizC8bppsMUA/__results___files/__results___64_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf0d2ef",
   "metadata": {},
   "source": [
    "![backpropagation-indepth.png](https://www.kaggleusercontent.com/kf/99705616/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..mS32FYLYwSsMSYO5vJAuZg.cXnDOwrGdtNgp8rFz2msenOptZMX1EkbxHcpivlCbaZ8bCZGfGkQWCS1XxhjMEFc_tfDkA-vchSRIrVp0_Det0nPjIY2ULmSaJl4K1qh4dgnYITyJKh5aLqu6e-cTzYTI3Z_9VH3RKrFoHRSgtRFOX5nHhD0W0N1twKKCXt9FOtoaPtILmefWCaj5jLwilnPc51SLU4YV-fhMkzPsBbXPS-NwCOlVGh9ro4D9BqmVeh2CLnfr4ob9C5oshfYqSUrj59HjhF596oM6HLqMv3Qvo1aee5YAWsQQaIHomc78ET9Pc08Cq_cgH-HYpMpxdBQSHTaTqexErxZZ3e4ChSZT5ehUQCX7pInt0ShsLOIO2VVDQUHzVx1Q1iGaZCQXpfY1xIFQpEw7MlW-z7J0RRFvaiscyyT6rZKdpGxQJBi2OReFAt7ubyRFEfMJ-MCzgkiOV9xtN84smPA5NQqCS16TVnQeBAUS79fkbpGXm2SbjSrtZgsiYVHmH6KFxrBDj8_W-Wg8p1u9sbZ-ry0ZjFRn4BUDy5XgK-wv-Lv749mWggYyGFRoO3FALi8DLCz7wdEVow-MndI3C3bdCUa2eUSfKctOCQntGoh6JvYEvVNRi2aACQJ5xHdyOOWEj84QCUOJd8vSSRoPZDMW6ycYZA9dYDOylHhwZlpPvl313VQeKA.ai3Ue7GYk7OizC8bppsMUA/__results___files/__results___65_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0473d90c",
   "metadata": {},
   "source": [
    "# Visualization of Neural Network using tensorspace.js"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28867a9c",
   "metadata": {},
   "source": [
    "![visual-png](https://cdn-images-1.medium.com/max/1000/1*_iuD-XPoKrBKG2TyftR8zA.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c5ea99",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
