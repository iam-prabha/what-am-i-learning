{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gradient Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**concept:** Gradient Boosting like AdaBoost, is a sequential ensemble method.However, instead of focusing on misclassified samples, it focuses on the \"residuals\" (the errors) from the previous model. Each new tree tries to correct the errors of the previous ensemble. It uses a gradient descent optimization algorithm to minimize the loss function.\n",
    "\n",
    "* **Goal:** Classification and Regression.\n",
    "\n",
    "* **How it works:** It builds an ensemble of weak learners (typically decision trees) sequentially, where each new learner corrects the errors made by the previous ones.\n",
    "\n",
    "**To Master Easier:** Think of a team trying to hit a target. The first team member takes a shot and misses. The next team member then tries to correct the error of the first shot, aiming closer to the target. This continues, with each team member trying to reduce the remaining error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-time Use Cases:**\n",
    "\n",
    "* **Web Search Ranking:** Optimizing search results by learning from user clicks.\n",
    "\n",
    "* **Financial Modeling:** Predicting stock movements, credit risk.\n",
    "\n",
    "* **Fraud Detection:** Highly effective due to its ability to capture complex patterns.\n",
    "\n",
    "* **Customer Lifetime Value (CLV) Prediction:** Predicting the future value of a customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation \n",
    "\n",
    "**Dataset:** A synthetic regression dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Gradient Boosting (Regressor) ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Gradient Boosting (Regressor) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1000, 20) (1000,)\n",
      "First 5 samples of X:\n",
      " [[-1.47716442 -0.60082964  0.6776371  -0.92015223 -1.71509335  0.9496415\n",
      "  -0.52976771  1.10269205  0.85192907  1.40431628 -0.87278495 -0.51029116\n",
      "  -0.0601545   0.80485594  0.1778298   0.9148678   0.24523585 -0.00767406\n",
      "   0.06578469  0.77969618]\n",
      " [ 0.69843278  1.12728112  1.38678066 -0.05261187  0.60105294  0.99164638\n",
      "  -0.30737032  1.10937157  0.29658384 -1.29296601 -0.61014975 -1.6540355\n",
      "  -0.11417027  1.24394192 -1.13918212  0.42656791 -1.01724537 -0.68274284\n",
      "   0.6952675  -0.71726391]\n",
      " [ 0.41934187  0.70902978  0.9885261  -1.75928702  1.62015868  0.8937618\n",
      "  -1.4571727  -1.907222   -1.30825326 -0.18194544  2.10098719 -0.79852039\n",
      "  -0.09515051 -0.1614279  -0.20177438  0.1556583   0.5970561   0.06382123\n",
      "   0.2565774   1.35389186]\n",
      " [ 0.36704679 -0.16066485  1.09672421 -1.23245439 -0.87713394 -1.33640242\n",
      "   0.72984304  0.4450427  -0.55393468 -0.26783943 -0.09789241  0.27981185\n",
      "  -0.20791231 -0.17932803  0.2789231   0.7293698  -2.55363102  0.68810802\n",
      "   0.88762296 -1.38437814]\n",
      " [ 0.47295356  0.60981252  1.28491678 -0.23071097 -1.3045489  -1.51327947\n",
      "   0.16968509  2.05480982  0.0220677  -0.2934314   1.34295633 -0.31660344\n",
      "   1.7533752   0.20039478  1.1490147  -2.04433048 -0.39942478 -0.47388287\n",
      "  -1.31809204  0.37771176]]\n",
      "First 5 samples of y: [   6.00404299  120.36272614  338.73928529 -360.19556056 -186.20193674]\n"
     ]
    }
   ],
   "source": [
    "# 1. create a synthetic dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=5, random_state=42)\n",
    "print(\"Dataset shape:\", X.shape, y.shape)\n",
    "print(\"First 5 samples of X:\\n\", X[:5])\n",
    "print(\"First 5 samples of y:\", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set shape: (800, 20) (800,)\n",
      "\n",
      "Testing set shape: (200, 20) (200,)\n",
      "\n",
      "First 5 samples of X_train:\n",
      " [[ 0.33607541 -0.37002247 -0.84209869  0.83533344 -2.21760908 -0.48716718\n",
      "   0.28815004  0.49955891  0.82614562 -1.08863484 -0.08119318  0.50011283\n",
      "  -0.3709311   0.35884807  0.88444244  0.32165266  2.80137299 -1.26727669\n",
      "   0.3987128  -0.27935025]\n",
      " [-0.43473497  1.15866702 -0.65079366  0.60554398 -0.2665473  -0.32368275\n",
      "  -1.12164624  0.00806676 -0.22878434 -0.85910427  0.52372777 -1.49519759\n",
      "  -1.59084586 -1.45121922 -0.00770865  0.16769153 -0.72453227 -0.75816382\n",
      "  -0.31629654  0.50808771]\n",
      " [ 1.51531809 -0.72839031  1.5891473   0.60378147 -0.23450754  0.68849597\n",
      "  -1.01268556  1.67427077 -0.48663119 -1.07066641  0.8452733  -0.54177324\n",
      "   0.70138989 -1.75995888 -0.90092112 -0.0102056  -0.78432097  0.50112928\n",
      "   0.06338339 -0.9125882 ]\n",
      " [ 0.10454225 -2.08706925 -0.68135161 -0.95110805 -0.37256228 -1.88586462\n",
      "  -1.83963787  1.67204177  0.59957863  0.0509137   0.07920293 -0.14468576\n",
      "  -1.08783755 -1.26488884  0.49721798 -0.86050733  0.17040099 -0.65905969\n",
      "  -0.60468085 -0.29412318]\n",
      " [-0.20108794  0.39412651  0.19380901 -0.22491954 -0.84671933 -1.69403928\n",
      "   1.91217805 -0.41987416 -0.50123796 -0.40446229 -0.9598503  -1.23695326\n",
      "   0.92807201 -0.93741379 -2.07495968  0.20188719 -1.31291298 -0.00640075\n",
      "   1.04002672 -0.12130867]]\n",
      "First 5 samples of y_train: [ -20.01835678 -170.44168885   61.7644767  -213.46605969 -327.30817193]\n"
     ]
    }
   ],
   "source": [
    "# 2. split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"\\nTraining set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"\\nTesting set shape:\", X_test.shape, y_test.shape)\n",
    "print(\"\\nFirst 5 samples of X_train:\\n\", X_train[:5])\n",
    "print(\"First 5 samples of y_train:\", y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# 3. Train the Model\n",
    "model_gbr = GradientBoostingRegressor(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42\n",
    ")\n",
    "model_gbr.fit(X_train, y_train)\n",
    "print(\"\\nModel trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 predictions: [179.86622906  74.36828605  94.75148776   1.12397176   4.66926085]\n"
     ]
    }
   ],
   "source": [
    "# 4. Make predictions\n",
    "y_pred_gbr = model_gbr.predict(X_test)\n",
    "print(\"\\nFirst 5 predictions:\", y_pred_gbr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Evaluate the Model\n",
    "rmse_gbr = np.sqrt(mean_squared_error(y_test, y_pred_gbr))\n",
    "r2_gbr = r2_score(y_test, y_pred_gbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 32.79\n",
      "R-squared: 0.97\n",
      "\n",
      "Discussion: Gradient Boosting builds an ensemble of weak learners (typically decision trees) sequentially. Each new tree attempts to correct the errors (residuals) of the previous ensemble. It's highly powerful and widely used for both regression and classification.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Root Mean Squared Error (RMSE): {rmse_gbr:.2f}\")\n",
    "print(f\"R-squared: {r2_gbr:.2f}\")\n",
    "\n",
    "print(\n",
    "    \"\\nDiscussion: Gradient Boosting builds an ensemble of weak learners (typically decision trees) sequentially. Each new tree attempts to correct the errors (residuals) of the previous ensemble. It's highly powerful and widely used for both regression and classification.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
