{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Adaptive Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**concept:** AdaBoost is another ensemble technique, but instead building trees independently like Random forest, it builds them sequentially. It focuses on the samples that were misclassified by the previous 'weak' learners (often simple decision stumps - very short trees). It gives more weight to thes misclassified samples in the next iteration, forcing the subsequent learners to pay more attention to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Goal:** Classification (primarily) and Regression.\n",
    "\n",
    "* **How it works:** It combines multiple \"weak\" learners to create a strong learner. It iteratively re-weights misclassified instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Master Easier:** Imagine you're a teacher and have a class struggling with a particular concept. AdaBoost is like focusing extra attention and different teaching methods on the students who are still struggling, ensuring they eventually grasp the concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-time Use Cases:**\n",
    "\n",
    "* **Face Detection:** Pioneering algorithm in real-time face detection (e.g., in cameras).\n",
    "\n",
    "* **Spam Filtering:** Improving the accuracy of spam detection.\n",
    "\n",
    "* **Medical Diagnosis:** Enhancing diagnostic accuracy by focusing on difficult cases.\n",
    "\n",
    "* **Customer Relationship Management (CRM):** Predicting customer behavior with higher precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaBoost Implementation**\n",
    "\n",
    "**Dataset:** A synthetic classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- AdaBoost Classifier ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier # AdaBoost often uses shallow trees\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "print('--- AdaBoost Classifier ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create a asynthetic classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 800\n",
      "Testing set size: 200\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "print(f'Training set size: {X_train.shape[0]}')\n",
    "print(f'Testing set size: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained successfully!\n",
      "Number of estimators: 100\n"
     ]
    }
   ],
   "source": [
    "# 2. Train the model \n",
    "model_ab = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
    "                              n_estimators=100,\n",
    "                              random_state=42,\n",
    "                              learning_rate=0.01)\n",
    "model_ab.fit(X_train, y_train)\n",
    "print('Model trained successfully!')\n",
    "print(f'Number of estimators: {model_ab.n_estimators}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 0 0 0 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 3. make predictions on the test set\n",
    "y_pred = model_ab.predict(X_test)\n",
    "print(f'Predictions: {y_pred[:10]}')  # Display first 10 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n",
      "Confusion Matrix: [[83 17]\n",
      " [15 85]]\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       100\n",
      "           1       0.83      0.85      0.84       100\n",
      "\n",
      "    accuracy                           0.84       200\n",
      "   macro avg       0.84      0.84      0.84       200\n",
      "weighted avg       0.84      0.84      0.84       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Evaluate the model\n",
    "accuracy_ab = accuracy_score(y_test, y_pred)\n",
    "conf_matrix_ab = confusion_matrix(y_test, y_pred)\n",
    "class_report_ab = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy_ab:.2f}')\n",
    "print(f'Confusion Matrix: {conf_matrix_ab}')\n",
    "print(f'Classification Report: {class_report_ab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discussion: AdaBoost sequentially builds 'weak' learners (like shallow decision trees). It focuses on misclassified samples by giving them more weight in subsequent iterations, leading to a strong combined classifier. It's good for improving the performance of simple models.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDiscussion: AdaBoost sequentially builds 'weak' learners (like shallow decision trees). It focuses on misclassified samples by giving them more weight in subsequent iterations, leading to a strong combined classifier. It's good for improving the performance of simple models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
